# Creating Q Tables and the Bellman Equation

## Objective
- To explain the **Bellman equation**.  
- To create **Q tables (state-action tables)**.

## Explanation with the Marshmallow Example ğŸ¬

### Scenario
A child either **eats the marshmallow now (action 1)** or **waits to earn two marshmallows (action 2)**.

### R (Reward) Table
- There are two columns: **Eat** and **Wait**  
- **Eating at any time â†’ 1 point**   
- **Waiting until the end â†’ 2 points** â³  
- At the beginning, the reward for the â€œwaitâ€ action is mostly **0** (no immediate reward).  
- Only near the final step does "wait" get the **actual reward = 2**.

### Problem
- The R table only contains immediate rewards.  
  â†’ The value of intermediate steps is unknown.  
  â†’ If we look only at R, **we cannot understand what to do at each step**. âŒ

## Solution: Bellman Equation

### Equation
\[
Q(s, a) = R(s, a) + \gamma \cdot \max Q(s', a')
\]

### Terms
- **S** â†’ Current state  
- **A** â†’ Set of possible actions  
- **I** â†’ A specific action chosen from the set  
- **R** â†’ Reward table (immediate rewards)  
- **Q** â†’ Learned Q table (past + estimated future rewards)  
- **Î³ (gamma)** â†’ Learning rate (how much future rewards are valued)

#### Effect of Gamma
- **Î³ = 0** â†’ Q = R, meaning **no learning** âŒ  
- **Î³ > 0** â†’ Future rewards are considered â†’ **Learning starts** âœ”ï¸

## How to Apply? ğŸ”„

1. **Start from the last step** (e.g., minute 7).  
   - â€œwaitâ€ action reward is **2 points**   
   - So: Q(7, wait) = 2

2. **Go back one step (minute 6):**  
   - R(6, wait) = 0 (no immediate reward)  
   - Q(6, wait) = R(6, wait) + Î³ Ã— max Q(7, a)  
   - max Q(7, a) = 2  
   - Î³ = 0.9  
     â†’ Q(6, wait) = 0 + 0.9 Ã— 2 = **1.8**

3. **Repeat backward for all steps.**  
   - Q table is filled **step by step**.

## What the Bellman Equation Provides

- It considers **not only immediate rewards** but also **future rewards**.  
  â†’ The agent evaluates not just the reward of the current action, but also the **best possible future rewards**.

- It enables learning by **updating Q-values (state-action values) step by step**.  
  â†’ This allows the agent to estimate **which action is more profitable in the long run** for each state.

- With the learning rate **gamma (Î³)**, it lets us control **how much importance is given to future rewards**.  
  â†’ The model can benefit from both **past experiences** and **future potential rewards** in a balanced way.

- As a result, the agent can discover the **optimal strategy** â€” the action sequence that leads to the **highest total reward**. ğŸ†

## Classic RL vs Deep RL ğŸ¤–

- **Classic RL:**  
  - Q values stored in a **table (Q-table)**.  
  - Computation gets harder as table grows.

- **Deep RL:**  
  - Q values predicted by a **neural network**.  
  - Better for large state-action spaces.
