# Reinforcement Learning Ã–ÄŸeleri

**Reinforcement Learning (PekiÅŸtirmeli Ã–ÄŸrenme)** sÃ¼recindeki Ã¼Ã§ temel Ã¶ÄŸe:
- **State (Durum)**
- **Action (Eylem)**
- **Reward (Ã–dÃ¼l)**

## ğŸ•¹ï¸ Ã–rnek: Pong Oyunu

1980'li yÄ±llarÄ±n meÅŸhur oyunu **Pong** Ã¼zerinden Ã¶rnek verilmiÅŸtir.

- **Eylemler (Actions)**: YukarÄ±, aÅŸaÄŸÄ± veya sabit kal.
- Her eylem belirli bir **zaman adÄ±mÄ±nda (T)** gerÃ§ekleÅŸtirilir.

Bu Ã¶rnekten yola Ã§Ä±karak ticaret (trading) uygulamasÄ±na geÃ§ilir:

- Bir yatÄ±rÄ±mcÄ± genellikle Ã¼Ã§ karar alabilir: **Buy (Al), Hold (Tut), Sell (Sat)**.
- Bu kararlar da her zaman adÄ±mÄ±nda bir eylemdir.

---

## ğŸ§¾ State (Durum) Nedir?

- **Pong'da**: Topun ve raketin pozisyonu = O anki **durum**.
- **Finansal piyasalarda**: Piyasa verileri (Ã¶rneÄŸin grafik gÃ¶rÃ¼nÃ¼mÃ¼) = Durum.

### Ã–rnek:

29 Haziran 2020 tarihinde Tesla'nÄ±n **OHLC (Open, High, Low, Close)** verileri alÄ±nÄ±r.

Ancak yalnÄ±zca bu sayÄ±lar yeterli **durum bilgisi** saÄŸlamaz. Daha anlamlÄ± bir yapÄ± gerekir.

---

## ğŸ” Daha Ä°yi Durum (State) Bilgisi NasÄ±l OluÅŸturulur?

### Basit yaklaÅŸÄ±m:
- Sadece son kapanÄ±ÅŸ fiyatÄ± ile bir Ã¶nceki kapanÄ±ÅŸ fiyatÄ± karÅŸÄ±laÅŸtÄ±rÄ±lÄ±r.

### Daha geliÅŸmiÅŸ yaklaÅŸÄ±m:
- Son N kapanÄ±ÅŸ fiyatÄ± dikkate alÄ±nÄ±r.
- Teknik gÃ¶stergeler (indikatÃ¶rler) eklenir:
  - **Hareketli Ortalamalar (Moving Averages)**
  - **RSI (Relative Strength Index)**
  - **Momentum**
  - **Volatilite**

---
## ğŸ¯ Action (Eylem) Nedir?

### Pongâ€™ta:
- KullanÄ±cÄ±nÄ±n yaptÄ±ÄŸÄ± hareket: yukarÄ±, aÅŸaÄŸÄ±, bekle

### Finansal Ä°ÅŸlemlerde:
- YatÄ±rÄ±mcÄ±nÄ±n her zaman adÄ±mÄ±nda verebileceÄŸi kararlar:
  - `Buy` (Al)
  - `Hold` (Bekle)
  - `Sell` (Sat)

### Detaylar:
- Her `state` (durum) gÃ¶zlemlendiÄŸinde, sistem bir `action` seÃ§er.
- SeÃ§ilen bu eylem, yatÄ±rÄ±m sonucunu belirler.
- Hangi eylemin seÃ§ileceÄŸi, geÃ§miÅŸ deneyimlerden ve o andaki `state`'ten etkilenir.

---

### Zaman dilimi farklÄ±lÄ±ÄŸÄ±:
- RSI indikatÃ¶rÃ¼ hem dakikalÄ±k, hem saatlik, hem de gÃ¼nlÃ¼k grafikte uygulanabilir.
- Bu sayede farklÄ± zaman Ã¶lÃ§eklerinden bilgi elde edilir.

TÃ¼m bu bilgiler birleÅŸtirilerek **zaman t'deki State vektÃ¶rÃ¼ (Sâ‚œ)** oluÅŸturulur.

---

## ğŸ‘¤ Ä°nsan YatÄ±rÄ±mcÄ± Analojisi

Bu durum (state), **insan yatÄ±rÄ±mcÄ±nÄ±n piyasaya bakÄ±p bir karar vermesi** gibi Ã§alÄ±ÅŸÄ±r.

- O anki fiyat, gÃ¶stergeler ve geÃ§miÅŸ tecrÃ¼beye dayanarak karar verilir.

---

## ğŸ“ˆ Ã–dÃ¼l (Reward) Ne Zaman ve NasÄ±l Belirlenir?

> Peki 30 Haziran'da yatÄ±rÄ±mcÄ± ne yapmalÄ±?

Verilecek kararÄ±n (action) sonucunda **Ã¶dÃ¼l (reward)** elde edilir.

Ama:

- Bu Ã¶dÃ¼l, yalnÄ±zca **gelecekte** belli olur.
- Ã–rneÄŸin: Hisse senedini ÅŸimdi almak â†’ Ä°leride fiyat artarsa Ã¶dÃ¼l, dÃ¼ÅŸerse ceza doÄŸar.

### Soru:
> "Bir aksiyonun Ã¶dÃ¼lÃ¼ nasÄ±l belirlenmeli?"

Bu soru, **Ã¶dÃ¼l fonksiyonu tasarÄ±mÄ± (reward function design)** kavramÄ±na gÃ¶tÃ¼rÃ¼r.  
Bu konu bir sonraki video dersinde ele alÄ±nacaktÄ±r.

---

## ğŸ§  SonuÃ§

- **Action** = Her adÄ±mda yapÄ±lacak seÃ§im (al, tut, sat gibi).
- **State** = O anki piyasa bilgisi ve gÃ¶stergeler.
- **Reward** = Eylemin uzun vadede getirdiÄŸi sonuÃ§.

Bu Ã¼Ã§ kavram Reinforcement Learningâ€™in temelini oluÅŸturur.  
Her karar, geÃ§miÅŸ durumlara ve kazanÄ±lan/kaÃ§Ä±rÄ±lan Ã¶dÃ¼llere gÃ¶re Ã¶ÄŸrenilir ve iyileÅŸtirilir.

---

